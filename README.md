pip install faiss-cpu




policies.pdf
     â†“ (PyMuPDF)
Clean Text
     â†“ (TextSplitter)
Chunks
     â†“ (OpenAI Embeddings)
Vectors
     â†“ (FAISS)
Vector DB
     â†“ (Query from user)
Similar Chunks
     â†“
OpenAI LLM â Final Answer (Chat)



SmartKnowledgeAssistant/
â”œâ”€â”€ data/ # Contains policies.pdf and other raw files
â”œâ”€â”€ scripts/ # All logic scripts
â”‚ â”œâ”€â”€ extract_data.py # Extracts text from PDFs
â”‚ â”œâ”€â”€ vectorize.py # Chunks text and creates vector store
â”‚ â””â”€â”€ chat_bot.py # Conversational RAG bot
â”œâ”€â”€ vector_store/ # FAISS vector database (auto-generated)
â”œâ”€â”€ .env # Stores OpenAI API key
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # Project documentation


policies.pdf â PyMuPDF â LangChain â FAISS â OpenAI LLM

ğŸ“¦ Tech Stack
LangChain (RAG logic): Breaks Text into Chunks Used in scripts/vectorize.py
FAISS (Semantic search)       
OpenAI GPT (LLM)
PyMuPDF (PDF text extraction)
dotenv (Environment management)



ğŸš€ Future Additions
Integrate Employee.csv analysis
Add Streamlit UI


where is it storing conversational memory? (conversational memory is stored in RAM (runtime) using this line)
in chatbot code- 
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
ConversationBufferMemory	Stores previous messages in a Python list (temporarily)

This memory is then passed into:
qa = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(temperature=0),
    retriever=retriever,
    memory=memory
)
LangChain automatically uses this memory to keep context of previous questions/answers.

in my project, conversational memory is stored in volatile memory (RAM) using LangChainâ€™s ConversationBufferMemory during a single chat session.
if i restart the app, memory is wiped.

